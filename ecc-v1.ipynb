{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, BatchNormalization\n",
    "from keras.layers import AveragePooling2D, Input, Flatten, MaxPooling2D\n",
    "from keras.layers import AveragePooling1D, MaxPooling1D, Conv1D, UpSampling1D, UpSampling2D\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras.callbacks import ReduceLROnPlateau\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 300 sequences of 100-bits sequences\n",
    "data_path = \"data/viterbi_dump_k100_n300.pk\"\n",
    "data = pickle.load(open(data_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.1, 0.05, 3), (0.1, 0.05, 5), (0.1, 0.1, 3), (0.1, 0.1, 5), (0.1, 0.15, 3), (0.1, 0.15, 5), (0.1, 0.2, 3), (0.1, 0.2, 5), (0.1, 0.3, 3), (0.1, 0.3, 5), (0.1, 0.5, 3), (0.1, 0.5, 5), (0.1, 0.8, 3), (0.1, 0.8, 5), (0.5, 0.05, 3), (0.5, 0.05, 5), (0.5, 0.1, 3), (0.5, 0.1, 5), (0.5, 0.15, 3), (0.5, 0.15, 5), (0.5, 0.2, 3), (0.5, 0.2, 5), (0.5, 0.3, 3), (0.5, 0.3, 5), (0.5, 0.5, 3), (0.5, 0.5, 5), (0.5, 0.8, 3), (0.5, 0.8, 5), (0.81, 0.05, 3), (0.81, 0.05, 5), (0.81, 0.1, 3), (0.81, 0.1, 5), (0.81, 0.15, 3), (0.81, 0.15, 5), (0.81, 0.2, 3), (0.81, 0.2, 5), (0.81, 0.3, 3), (0.81, 0.3, 5), (0.81, 0.5, 3), (0.81, 0.5, 5), (0.81, 0.8, 3), (0.81, 0.8, 5)]\n"
     ]
    }
   ],
   "source": [
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_seqs = list()\n",
    "noisy_seqs = list()\n",
    "viterbi_decoded_seqs = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, d in data.items():\n",
    "#     if k[1] > 0.5:\n",
    "#         continue\n",
    "    message_seqs.append(d[0])\n",
    "    noisy_seqs.append(d[1])\n",
    "    viterbi_decoded_seqs.append(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_seqs = np.concatenate(np.array(message_seqs))\n",
    "noisy_seqs = np.concatenate(np.array(noisy_seqs))\n",
    "viterbi_decoded_seqs = np.concatenate(np.array(viterbi_decoded_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12600, 100)\n",
      "(12600,)\n",
      "(12600,)\n"
     ]
    }
   ],
   "source": [
    "print(message_seqs.shape)\n",
    "print(noisy_seqs.shape)\n",
    "print(viterbi_decoded_seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([204, 208])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noisy sequences have two lengths\n",
    "np.unique([len(x) for x in noisy_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102, 104])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viterbi_decoded sequences have two lengths\n",
    "np.unique([len(x) for x in viterbi_decoded_seqs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems that noisy sequences and viterbi decoding outputs various length sequences, but generally the additional bytes at the end are preample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(low=0, high=len(message_seqs))\n",
    "message_seqs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_decoded_seqs[idx].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_loss(ground_truth, decodes):\n",
    "    if len(ground_truth) != len(decodes):\n",
    "        raise ValueError(\"ground_truth and decodes have different length: {0} and {1}\"\n",
    "                            .format(len(ground_truth), len(decodes)))\n",
    "    l = 0.0\n",
    "    for g, d in zip(ground_truth, decodes):\n",
    "        l += sum(abs(g - d[:len(g)]))\n",
    "    return l / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.90396825396825"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss(message_seqs, viterbi_decoded_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Length Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([204, 208])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noisy sequences have two lengths\n",
    "np.unique([len(x) for x in noisy_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([102, 104])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viterbi_decoded sequences have two lengths\n",
    "np.unique([len(x) for x in viterbi_decoded_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standarlize length of noisy_inputs\n",
    "noisy_seqs_sameLength = np.array([x if len(x) == 208 else np.concatenate([x, [0,0,0,0]]) \n",
    "                                      for x in noisy_seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12600, 100)\n",
      "(12600, 208)\n"
     ]
    }
   ],
   "source": [
    "# Now input lengths are standardized\n",
    "print(message_seqs.shape)\n",
    "print(noisy_seqs_sameLength.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = noisy_seqs_sameLength.astype(\"float64\")\n",
    "Y = message_seqs.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2981, 6085, 4588, ..., 1024, 7583, 6264])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicies = np.arange(len(X))\n",
    "np.random.shuffle(indicies)\n",
    "indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split = 0.2\n",
    "split_idx = int(len(X) * (1 - train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X[indicies[:split_idx]], X[indicies[split_idx:]]\n",
    "Y_train, Y_test = Y[indicies[:split_idx]], Y[indicies[split_idx:]]\n",
    "viterbi_decoded_train = [x[:100] for x in viterbi_decoded_seqs[indicies[:split_idx]]]\n",
    "viterbi_decoded_test = [x[:100] for x in viterbi_decoded_seqs[indicies[split_idx:]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64,  activation=None, input_shape=(208,), \n",
    "                kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(128, activation=None, \n",
    "                kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(256, activation=None, \n",
    "                kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(128, activation=None, \n",
    "                kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(100, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=32, epochs=25, verbose=1, \n",
    "          validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def threshold(preds):\n",
    "    result = []\n",
    "    for pred in preds:\n",
    "        result.append([1 if x >= 0.5 else 0 for x in pred])\n",
    "    return np.array(result).astype(\"float64\")\n",
    "def test(input_x, target_label, viterbi_output, test_model):\n",
    "    viterbi_avg_loss = avg_loss(target_label, viterbi_output)\n",
    "    model_avg_loss = avg_loss(target_label, threshold(test_model.predict(input_x)))\n",
    "    print(\"average loss for viterbi decoding is: {0:.4f}; acc {1:.4f}\".format(\n",
    "                viterbi_avg_loss, 1 - viterbi_avg_loss/input_x.shape[1]))\n",
    "    print(\"average loss for model is: {0:.4f}; acc {1:.4f}\".format(\n",
    "                model_avg_loss, 1 - model_avg_loss/input_x.shape[1]))\n",
    "    print(\"## Random Example ##\")\n",
    "    test_idx = np.random.randint(0, high=len(input_x))\n",
    "    test_truth = target_label[test_idx]\n",
    "    #test_pred = test_model.predict(input_x[test_idx:test_idx+1])[0]\n",
    "    test_pred_thresholded = threshold(test_model.predict(input_x[test_idx:test_idx+1]))[0]\n",
    "    diff = abs(test_pred_thresholded - test_truth)\n",
    "    print(\"ground_truth\\n\", test_truth)\n",
    "    #print(\"prediction\\n\", test_pred)\n",
    "    print(\"prediction_thresholded\\n\", test_pred_thresholded)\n",
    "    print(\"DIFFERENCE (acc {0:.4f}%)\\n\".format(1-sum(diff)/len(test_pred_thresholded)), diff)\n",
    "    print(\"dummy baseline (set every bit to the most occuring bit) can get {0}% accurate\".format(\n",
    "                max(sum(test_truth), 100-sum(test_truth))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(X_test, Y_test, viterbi_decoded_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test various feedforward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FF_MODELS = [\n",
    "    [32, 100],\n",
    "    [64, 100],\n",
    "    [128, 100],\n",
    "    [256, 100],\n",
    "    [32, 32, 100],\n",
    "    [32, 64, 100],\n",
    "    [64, 64, 100],\n",
    "    [64, 128, 100],\n",
    "    [128, 128, 100],\n",
    "    [128, 256, 100],\n",
    "    [256, 256, 100],\n",
    "    [32,128,128,100],\n",
    "    [32,128,256,100],\n",
    "    [32,256,256,100],\n",
    "    [32,128,256,128,100],\n",
    "    [32,128,128,128,100],\n",
    "    [1024,512,100],\n",
    "    [512,256,100],\n",
    "    [256,128,100],\n",
    "    [128,100,100]\n",
    "]\n",
    "\n",
    "ACTIVATIONS = [None, \"relu\"]\n",
    "L2 = 1e-7\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "VALIDATION_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fnn_build_and_evaluate_performance(layers, activation=\"None\", verbose=2, \n",
    "                                   input_shape=(208,), validation_split=VALIDATION_RATIO):\n",
    "    if layers[-1] != 100:\n",
    "        raise ValueError(\"last output dimension has to be 100\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    for i, hidden_units in enumerate(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(hidden_units, activation=activation, \n",
    "                            input_shape=input_shape, kernel_regularizer=regularizers.l2(L2)))\n",
    "        elif i == len(layers) - 1:\n",
    "            model.add(Dense(hidden_units, activation=\"sigmoid\",\n",
    "                           kernel_regularizer=regularizers.l2(L2)))\n",
    "        else:\n",
    "            model.add(Dense(100, activation=activation, kernel_regularizer=regularizers.l2(L2)))\n",
    "            \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=verbose, \n",
    "              validation_split=validation_split, shuffle=True)\n",
    "    \n",
    "    return model, avg_loss(Y_test, threshold(model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name(lst, act):\n",
    "    return \"[{0}]-{1}\".format(\",\".join([str(x) for x in lst]), act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32,100]-None loss: 39.1067\n",
      "[32,100]-relu loss: 39.3381\n",
      "[64,100]-None loss: 40.0048\n",
      "[64,100]-relu loss: 39.3159\n",
      "[128,100]-None loss: 39.7810\n",
      "[128,100]-relu loss: 38.8710\n",
      "[256,100]-None loss: 39.4921\n",
      "[256,100]-relu loss: 39.0508\n",
      "[32,32,100]-None loss: 39.0020\n",
      "[32,32,100]-relu loss: 39.1937\n",
      "[32,64,100]-None loss: 39.2079\n",
      "[32,64,100]-relu loss: 39.2833\n",
      "[64,64,100]-None loss: 40.3151\n",
      "[64,64,100]-relu loss: 36.6774\n",
      "[64,128,100]-None loss: 39.8548\n",
      "[64,128,100]-relu loss: 36.5746\n",
      "[128,128,100]-None loss: 39.2095\n",
      "[128,128,100]-relu loss: 36.5591\n",
      "[128,256,100]-None loss: 39.9813\n",
      "[128,256,100]-relu loss: 36.6901\n",
      "[256,256,100]-None loss: 39.6996\n",
      "[256,256,100]-relu loss: 37.2758\n",
      "[32,128,128,100]-None loss: 38.5710\n",
      "[32,128,128,100]-relu loss: 37.6992\n",
      "[32,128,256,100]-None loss: 38.8147\n",
      "[32,128,256,100]-relu loss: 36.9992\n",
      "[32,256,256,100]-None loss: 38.5742\n",
      "[32,256,256,100]-relu loss: 37.4948\n",
      "[32,128,256,128,100]-None loss: 39.4413\n",
      "[32,128,256,128,100]-relu loss: 36.9464\n",
      "[32,128,128,128,100]-None loss: 38.7067\n",
      "[32,128,128,128,100]-relu loss: 36.7889\n",
      "[1024,512,100]-None loss: 39.4456\n",
      "[1024,512,100]-relu loss: 36.9075\n",
      "[512,256,100]-None loss: 38.6782\n",
      "[512,256,100]-relu loss: 37.6750\n",
      "[256,128,100]-None loss: 39.1917\n",
      "[256,128,100]-relu loss: 36.8278\n",
      "[128,100,100]-None loss: 39.7758\n",
      "[128,100,100]-relu loss: 37.3627\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for m in FF_MODELS:\n",
    "    for a in ACTIVATIONS:\n",
    "        k = name(m, a)\n",
    "        model, loss = fnn_build_and_evaluate_performance(m, activation=a, verbose=0)\n",
    "        results[k] = (model, loss)\n",
    "        print(\"{0} loss: {1:.4f}\".format(k, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss for viterbi decoding is: 33.9929; acc 0.8366\n",
      "average loss for model is: 36.5591; acc 0.8242\n",
      "## Random Example ##\n",
      "ground_truth\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1.]\n",
      "prediction_thresholded\n",
      " [1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0.]\n",
      "DIFFERENCE (acc 0.4600%)\n",
      " [0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1.]\n",
      "dummy baseline (set every bit to the most occuring bit) can get 82.0% accurate\n"
     ]
    }
   ],
   "source": [
    "test(X_test, Y_test, viterbi_decoded_test, results[\"[128,128,100]-relu\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = results[\"[128,128,100]-relu\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.save(\"models/ecc-fnn-128-128-100-relu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8064 samples, validate on 2016 samples\n",
      "Epoch 1/100\n",
      " - 9s - loss: 0.2491 - acc: 0.0097 - val_loss: 0.2454 - val_acc: 0.0045\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.2363 - acc: 0.0403 - val_loss: 0.2295 - val_acc: 0.0119\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2213 - acc: 0.0435 - val_loss: 0.2236 - val_acc: 0.0565\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2133 - acc: 0.0668 - val_loss: 0.2196 - val_acc: 0.0570\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2062 - acc: 0.0809 - val_loss: 0.2149 - val_acc: 0.0799\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1986 - acc: 0.1173 - val_loss: 0.2139 - val_acc: 0.0283\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1930 - acc: 0.0784 - val_loss: 0.2119 - val_acc: 0.0769\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1880 - acc: 0.1017 - val_loss: 0.2117 - val_acc: 0.0655\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1845 - acc: 0.0996 - val_loss: 0.2119 - val_acc: 0.0694\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1813 - acc: 0.0914 - val_loss: 0.2128 - val_acc: 0.0759\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1787 - acc: 0.0720 - val_loss: 0.2160 - val_acc: 0.1195\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.1765 - acc: 0.0952 - val_loss: 0.2155 - val_acc: 0.1280\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.1744 - acc: 0.1028 - val_loss: 0.2163 - val_acc: 0.0774\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.1727 - acc: 0.1114 - val_loss: 0.2172 - val_acc: 0.0794\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.1713 - acc: 0.1238 - val_loss: 0.2181 - val_acc: 0.0531\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.1699 - acc: 0.1055 - val_loss: 0.2185 - val_acc: 0.1136\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.1688 - acc: 0.1221 - val_loss: 0.2214 - val_acc: 0.1022\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.1678 - acc: 0.1381 - val_loss: 0.2206 - val_acc: 0.0992\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1670 - acc: 0.1174 - val_loss: 0.2205 - val_acc: 0.1364\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.1664 - acc: 0.1334 - val_loss: 0.2207 - val_acc: 0.1300\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.1657 - acc: 0.1404 - val_loss: 0.2214 - val_acc: 0.1171\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1652 - acc: 0.1318 - val_loss: 0.2222 - val_acc: 0.1106\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1648 - acc: 0.1384 - val_loss: 0.2225 - val_acc: 0.1225\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1644 - acc: 0.1385 - val_loss: 0.2232 - val_acc: 0.1195\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1640 - acc: 0.1334 - val_loss: 0.2234 - val_acc: 0.1181\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1637 - acc: 0.1518 - val_loss: 0.2241 - val_acc: 0.0878\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1634 - acc: 0.1357 - val_loss: 0.2244 - val_acc: 0.1344\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1632 - acc: 0.1339 - val_loss: 0.2249 - val_acc: 0.1443\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1629 - acc: 0.1363 - val_loss: 0.2249 - val_acc: 0.1220\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1627 - acc: 0.1324 - val_loss: 0.2251 - val_acc: 0.1230\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1626 - acc: 0.1384 - val_loss: 0.2254 - val_acc: 0.1344\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1625 - acc: 0.1264 - val_loss: 0.2258 - val_acc: 0.1364\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1624 - acc: 0.1472 - val_loss: 0.2265 - val_acc: 0.1161\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1621 - acc: 0.1319 - val_loss: 0.2259 - val_acc: 0.0893\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.1619 - acc: 0.1308 - val_loss: 0.2261 - val_acc: 0.0843\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1617 - acc: 0.1283 - val_loss: 0.2265 - val_acc: 0.1270\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1615 - acc: 0.1370 - val_loss: 0.2266 - val_acc: 0.1225\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.1615 - acc: 0.1302 - val_loss: 0.2265 - val_acc: 0.1007\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1613 - acc: 0.1297 - val_loss: 0.2268 - val_acc: 0.1215\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1612 - acc: 0.1348 - val_loss: 0.2271 - val_acc: 0.1121\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1610 - acc: 0.1355 - val_loss: 0.2280 - val_acc: 0.1176\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.1608 - acc: 0.1312 - val_loss: 0.2277 - val_acc: 0.1186\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1612 - acc: 0.1311 - val_loss: 0.2279 - val_acc: 0.1071\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.1609 - acc: 0.1291 - val_loss: 0.2285 - val_acc: 0.1052\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1606 - acc: 0.1369 - val_loss: 0.2286 - val_acc: 0.0893\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.1602 - acc: 0.1267 - val_loss: 0.2287 - val_acc: 0.1081\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.1600 - acc: 0.1333 - val_loss: 0.2290 - val_acc: 0.0972\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.1598 - acc: 0.1272 - val_loss: 0.2288 - val_acc: 0.1161\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.1597 - acc: 0.1345 - val_loss: 0.2302 - val_acc: 0.1171\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.1595 - acc: 0.1280 - val_loss: 0.2301 - val_acc: 0.1086\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.1594 - acc: 0.1295 - val_loss: 0.2302 - val_acc: 0.0933\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.1592 - acc: 0.1342 - val_loss: 0.2301 - val_acc: 0.0977\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.1590 - acc: 0.1295 - val_loss: 0.2300 - val_acc: 0.1012\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.1588 - acc: 0.1338 - val_loss: 0.2307 - val_acc: 0.1161\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.1587 - acc: 0.1297 - val_loss: 0.2314 - val_acc: 0.0838\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.1585 - acc: 0.1312 - val_loss: 0.2317 - val_acc: 0.1002\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.1583 - acc: 0.1285 - val_loss: 0.2325 - val_acc: 0.0982\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.1582 - acc: 0.1292 - val_loss: 0.2315 - val_acc: 0.1062\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.1582 - acc: 0.1281 - val_loss: 0.2323 - val_acc: 0.0893\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.1583 - acc: 0.1332 - val_loss: 0.2320 - val_acc: 0.0937\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.1579 - acc: 0.1281 - val_loss: 0.2330 - val_acc: 0.0938\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.1576 - acc: 0.1301 - val_loss: 0.2330 - val_acc: 0.1057\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.1575 - acc: 0.1257 - val_loss: 0.2331 - val_acc: 0.0957\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.1574 - acc: 0.1319 - val_loss: 0.2342 - val_acc: 0.1012\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.1574 - acc: 0.1298 - val_loss: 0.2334 - val_acc: 0.0903\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.1572 - acc: 0.1310 - val_loss: 0.2338 - val_acc: 0.0818\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.1571 - acc: 0.1251 - val_loss: 0.2347 - val_acc: 0.0997\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.1569 - acc: 0.1262 - val_loss: 0.2342 - val_acc: 0.1062\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.1568 - acc: 0.1333 - val_loss: 0.2347 - val_acc: 0.1027\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1567 - acc: 0.1282 - val_loss: 0.2347 - val_acc: 0.0933\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1566 - acc: 0.1281 - val_loss: 0.2355 - val_acc: 0.0942\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1566 - acc: 0.1323 - val_loss: 0.2356 - val_acc: 0.0868\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1565 - acc: 0.1231 - val_loss: 0.2354 - val_acc: 0.0957\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1563 - acc: 0.1244 - val_loss: 0.2353 - val_acc: 0.0923\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1561 - acc: 0.1286 - val_loss: 0.2363 - val_acc: 0.1052\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1560 - acc: 0.1239 - val_loss: 0.2361 - val_acc: 0.1027\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1561 - acc: 0.1291 - val_loss: 0.2363 - val_acc: 0.0923\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1559 - acc: 0.1235 - val_loss: 0.2364 - val_acc: 0.1002\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1555 - acc: 0.1285 - val_loss: 0.2368 - val_acc: 0.1022\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1554 - acc: 0.1283 - val_loss: 0.2369 - val_acc: 0.0957\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1552 - acc: 0.1296 - val_loss: 0.2373 - val_acc: 0.0838\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1551 - acc: 0.1264 - val_loss: 0.2371 - val_acc: 0.1022\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1551 - acc: 0.1270 - val_loss: 0.2373 - val_acc: 0.1066\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1550 - acc: 0.1266 - val_loss: 0.2377 - val_acc: 0.0997\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1549 - acc: 0.1310 - val_loss: 0.2377 - val_acc: 0.1012\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1548 - acc: 0.1307 - val_loss: 0.2387 - val_acc: 0.0982\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1547 - acc: 0.1288 - val_loss: 0.2387 - val_acc: 0.0853\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1546 - acc: 0.1261 - val_loss: 0.2388 - val_acc: 0.0923\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1544 - acc: 0.1261 - val_loss: 0.2389 - val_acc: 0.0997\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1543 - acc: 0.1300 - val_loss: 0.2389 - val_acc: 0.0878\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1542 - acc: 0.1257 - val_loss: 0.2389 - val_acc: 0.0952\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1541 - acc: 0.1307 - val_loss: 0.2394 - val_acc: 0.0962\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.1541 - acc: 0.1257 - val_loss: 0.2400 - val_acc: 0.1022\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.1540 - acc: 0.1286 - val_loss: 0.2401 - val_acc: 0.1032\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.1540 - acc: 0.1326 - val_loss: 0.2399 - val_acc: 0.0878\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.1538 - acc: 0.1288 - val_loss: 0.2401 - val_acc: 0.1161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      " - 0s - loss: 0.1537 - acc: 0.1317 - val_loss: 0.2402 - val_acc: 0.0942\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.1536 - acc: 0.1312 - val_loss: 0.2416 - val_acc: 0.1032\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.1536 - acc: 0.1291 - val_loss: 0.2410 - val_acc: 0.0972\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.1535 - acc: 0.1298 - val_loss: 0.2407 - val_acc: 0.0977\n"
     ]
    }
   ],
   "source": [
    "L2 = 1e-7\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "VALIDATION_RATIO = 0.2\n",
    "model, loss = fnn_build_and_evaluate_performance([100,100,100], activation=\"relu\", verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss for viterbi decoding is: 33.9929; acc 0.8366\n",
      "average loss for model is: 36.7734; acc 0.8232\n",
      "## Random Example ##\n",
      "ground_truth\n",
      " [1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 1. 0. 0. 1.]\n",
      "prediction_thresholded\n",
      " [1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1.]\n",
      "DIFFERENCE (acc 0.4600%)\n",
      " [0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0.]\n",
      "dummy baseline (set every bit to the most occuring bit) can get 50.0% accurate\n"
     ]
    }
   ],
   "source": [
    "test(X_test, Y_test, viterbi_decoded_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation: The model overfits always by guessing everything to be the value of the most dominating digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.reshape((-1, 13, 16, 1))\n",
    "Y_train_cnn = Y_train.reshape((-1, 100))\n",
    "\n",
    "X_test_cnn = X_test.reshape((-1, 13, 16, 1))\n",
    "Y_test_cnn = Y_test.reshape((-1, 100))\n",
    "\n",
    "input_shape=(13, 16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACTIVATION = \"linear\"\n",
    "PADDING = \"same\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=1,padding=PADDING, \n",
    "                 activation=ACTIVATION, input_shape=input_shape))\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=1, padding=PADDING, activation=ACTIVATION))\n",
    "\n",
    "model.add(AveragePooling2D(pool_size=4, strides=None, padding=PADDING))\n",
    "#model.add(MaxPooling2D(pool_size=4, strides=None, padding='same'))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding=PADDING, activation=ACTIVATION))\n",
    "model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding=PADDING, activation=ACTIVATION))\n",
    "\n",
    "#model.add(AveragePooling2D(pool_size=3, strides=None, padding=PADDING))\n",
    "#model.add(MaxPooling2D(pool_size=2, strides=None, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation=ACTIVATION))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(100, activation=ACTIVATION))\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(100, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8064 samples, validate on 2016 samples\n",
      "Epoch 1/40\n",
      " - 11s - loss: 0.2446 - acc: 0.0069 - val_loss: 0.2381 - val_acc: 0.0025\n",
      "Epoch 2/40\n",
      " - 3s - loss: 0.2385 - acc: 0.0123 - val_loss: 0.2347 - val_acc: 0.0055\n",
      "Epoch 3/40\n",
      " - 3s - loss: 0.2361 - acc: 0.0445 - val_loss: 0.2334 - val_acc: 0.0541\n",
      "Epoch 4/40\n",
      " - 3s - loss: 0.2349 - acc: 0.0755 - val_loss: 0.2331 - val_acc: 0.1111\n",
      "Epoch 5/40\n",
      " - 3s - loss: 0.2344 - acc: 0.0919 - val_loss: 0.2333 - val_acc: 0.0694\n",
      "Epoch 6/40\n",
      " - 3s - loss: 0.2343 - acc: 0.0851 - val_loss: 0.2322 - val_acc: 0.1726\n",
      "Epoch 7/40\n",
      " - 3s - loss: 0.2334 - acc: 0.0991 - val_loss: 0.2323 - val_acc: 0.0997\n",
      "Epoch 8/40\n",
      " - 4s - loss: 0.2334 - acc: 0.0776 - val_loss: 0.2349 - val_acc: 0.0719\n",
      "Epoch 9/40\n",
      " - 3s - loss: 0.2333 - acc: 0.1022 - val_loss: 0.2322 - val_acc: 0.0645\n",
      "Epoch 10/40\n",
      " - 3s - loss: 0.2331 - acc: 0.0787 - val_loss: 0.2321 - val_acc: 0.1424\n",
      "Epoch 11/40\n",
      " - 3s - loss: 0.2328 - acc: 0.0910 - val_loss: 0.2318 - val_acc: 0.1721\n",
      "Epoch 12/40\n",
      " - 3s - loss: 0.2328 - acc: 0.1115 - val_loss: 0.2325 - val_acc: 0.0873\n",
      "Epoch 13/40\n",
      " - 3s - loss: 0.2325 - acc: 0.0925 - val_loss: 0.2335 - val_acc: 0.0754\n",
      "Epoch 14/40\n",
      " - 3s - loss: 0.2326 - acc: 0.1128 - val_loss: 0.2320 - val_acc: 0.1935\n",
      "Epoch 15/40\n",
      " - 3s - loss: 0.2320 - acc: 0.1085 - val_loss: 0.2359 - val_acc: 0.0843\n",
      "Epoch 16/40\n",
      " - 4s - loss: 0.2330 - acc: 0.1004 - val_loss: 0.2326 - val_acc: 0.0501\n",
      "Epoch 17/40\n",
      " - 3s - loss: 0.2321 - acc: 0.0992 - val_loss: 0.2330 - val_acc: 0.1086\n",
      "Epoch 18/40\n",
      " - 3s - loss: 0.2321 - acc: 0.1032 - val_loss: 0.2316 - val_acc: 0.1007\n",
      "Epoch 19/40\n",
      " - 3s - loss: 0.2321 - acc: 0.1128 - val_loss: 0.2319 - val_acc: 0.1329\n",
      "Epoch 20/40\n",
      " - 3s - loss: 0.2323 - acc: 0.1282 - val_loss: 0.2322 - val_acc: 0.1121\n",
      "Epoch 21/40\n",
      " - 3s - loss: 0.2316 - acc: 0.1137 - val_loss: 0.2328 - val_acc: 0.1131\n",
      "Epoch 22/40\n",
      " - 3s - loss: 0.2319 - acc: 0.1117 - val_loss: 0.2321 - val_acc: 0.1458\n",
      "Epoch 23/40\n",
      " - 3s - loss: 0.2318 - acc: 0.1198 - val_loss: 0.2322 - val_acc: 0.1136\n",
      "Epoch 24/40\n",
      " - 3s - loss: 0.2317 - acc: 0.1187 - val_loss: 0.2315 - val_acc: 0.1027\n",
      "Epoch 25/40\n",
      " - 3s - loss: 0.2316 - acc: 0.1198 - val_loss: 0.2334 - val_acc: 0.0561\n",
      "Epoch 26/40\n",
      " - 3s - loss: 0.2318 - acc: 0.1236 - val_loss: 0.2344 - val_acc: 0.2297\n",
      "Epoch 27/40\n",
      " - 3s - loss: 0.2319 - acc: 0.1357 - val_loss: 0.2315 - val_acc: 0.0853\n",
      "Epoch 28/40\n",
      " - 3s - loss: 0.2316 - acc: 0.1276 - val_loss: 0.2319 - val_acc: 0.1825\n",
      "Epoch 29/40\n",
      " - 3s - loss: 0.2316 - acc: 0.1331 - val_loss: 0.2320 - val_acc: 0.1920\n",
      "Epoch 30/40\n",
      " - 3s - loss: 0.2316 - acc: 0.1310 - val_loss: 0.2318 - val_acc: 0.0972\n",
      "Epoch 31/40\n",
      " - 3s - loss: 0.2313 - acc: 0.1249 - val_loss: 0.2320 - val_acc: 0.1066\n",
      "Epoch 32/40\n",
      " - 3s - loss: 0.2315 - acc: 0.1285 - val_loss: 0.2321 - val_acc: 0.1190\n",
      "Epoch 33/40\n",
      " - 3s - loss: 0.2314 - acc: 0.1379 - val_loss: 0.2337 - val_acc: 0.1255\n",
      "Epoch 34/40\n",
      " - 3s - loss: 0.2316 - acc: 0.1385 - val_loss: 0.2323 - val_acc: 0.1166\n",
      "Epoch 35/40\n",
      " - 3s - loss: 0.2314 - acc: 0.1357 - val_loss: 0.2314 - val_acc: 0.0893\n",
      "Epoch 36/40\n",
      " - 3s - loss: 0.2317 - acc: 0.1504 - val_loss: 0.2322 - val_acc: 0.0754\n",
      "Epoch 37/40\n",
      " - 3s - loss: 0.2315 - acc: 0.1446 - val_loss: 0.2318 - val_acc: 0.1210\n",
      "Epoch 38/40\n",
      " - 3s - loss: 0.2310 - acc: 0.1472 - val_loss: 0.2319 - val_acc: 0.1493\n",
      "Epoch 39/40\n",
      " - 3s - loss: 0.2322 - acc: 0.1482 - val_loss: 0.2308 - val_acc: 0.1538\n",
      "Epoch 40/40\n",
      " - 3s - loss: 0.2315 - acc: 0.1406 - val_loss: 0.2327 - val_acc: 0.0868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x189aa7f60>"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_cnn, Y_train_cnn, batch_size=32, epochs=40, verbose=2, \n",
    "          validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      " [0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "pred\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1.]\n",
      "accu 70%\n"
     ]
    }
   ],
   "source": [
    "test_idx = np.random.randint(0, high=len(X_test_cnn))\n",
    "tmp = model.predict(X_test_cnn[test_idx:test_idx+1]).reshape(1,100)\n",
    "\n",
    "test_true = Y_test_cnn[test_idx:test_idx+1][0]\n",
    "test_pred = threshold(tmp)[0]\n",
    "\n",
    "print(\"true\\n\", test_true)\n",
    "print(\"pred\\n\", test_pred)\n",
    "print(\"accu {0}%\".format(sum(test_true == test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tmodel.save(\"models/ecc-cnn1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18efc32e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"percent of 1s in the groundtruth code\")\n",
    "_ = plt.boxplot([sum(x)/100 for x in Y_test], vert=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMYZbkoox3JJUjOGWpGL+D9JN5dnNE2MkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18efc3208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"percent of 1s in the model prediction\")\n",
    "_ = plt.boxplot([sum(x)/100 for x in model.predict(X_test_cnn)], vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_cnn = X_train.reshape((-1, 208, 1)) # 208\n",
    "Y_train_cnn = Y_train.reshape((-1, 100))\n",
    "\n",
    "X_test_cnn = X_test.reshape((-1, 208, 1))\n",
    "Y_test_cnn = Y_test.reshape((-1, 100))\n",
    "\n",
    "input_shape=(208, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION = \"relu\"\n",
    "PADDING = \"same\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 208 x 1\n",
    "model.add(Conv1D(filters=16, kernel_size=3, strides=1,padding=PADDING, \n",
    "                 activation=ACTIVATION, input_shape=input_shape))\n",
    "# 208 x 16\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='same'))\n",
    "# 104 x 16\n",
    "model.add(Conv1D(filters=8, kernel_size=3, strides=1, padding=PADDING, activation=ACTIVATION))\n",
    "# 104 x 8\n",
    "model.add(MaxPooling1D(pool_size=2, strides=None, padding='same'))\n",
    "# 52 x 8\n",
    "\n",
    "model.add(UpSampling1D(size=2))\n",
    "# 104 x 8\n",
    "model.add(Conv1D(filters=8, kernel_size=3, strides=1, padding=PADDING, activation=ACTIVATION))\n",
    "# 104 x 8\n",
    "model.add(UpSampling1D(size=2))\n",
    "# 208 x 8\n",
    "model.add(Conv1D(filters=16, kernel_size=3, strides=1, padding=PADDING, activation=ACTIVATION))\n",
    "# 208 x 16\n",
    "\n",
    "model.add(Flatten())\n",
    "# 3328\n",
    "\n",
    "model.add(Dense(1024, activation=ACTIVATION))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(526, activation=ACTIVATION))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation=ACTIVATION))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8064 samples, validate on 2016 samples\n",
      "Epoch 1/20\n",
      " - 16s - loss: 0.2357 - acc: 0.0105 - val_loss: 0.2209 - val_acc: 4.9603e-04\n",
      "Epoch 2/20\n",
      " - 15s - loss: 0.2183 - acc: 0.0071 - val_loss: 0.2155 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      " - 15s - loss: 0.2137 - acc: 0.0117 - val_loss: 0.2148 - val_acc: 0.0223\n",
      "Epoch 4/20\n",
      " - 15s - loss: 0.2111 - acc: 0.0081 - val_loss: 0.2141 - val_acc: 0.0020\n",
      "Epoch 5/20\n",
      " - 16s - loss: 0.2075 - acc: 0.0078 - val_loss: 0.2118 - val_acc: 0.0020\n",
      "Epoch 6/20\n",
      " - 16s - loss: 0.2036 - acc: 0.0057 - val_loss: 0.2087 - val_acc: 9.9206e-04\n",
      "Epoch 7/20\n",
      " - 15s - loss: 0.1983 - acc: 0.0114 - val_loss: 0.2103 - val_acc: 0.0020\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c082ecb9b38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train_cnn, Y_train_cnn, batch_size=32, epochs=20, verbose=2, \n\u001b[0;32m----> 3\u001b[0;31m           validation_split=0.2, shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michael-tu/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.fit(X_train_cnn, Y_train_cnn, batch_size=32, epochs=20, verbose=2, \n",
    "          validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
