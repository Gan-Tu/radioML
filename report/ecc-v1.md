# General Comments

This report corresponds to [ecc-v1 notebook](https://github.com/Michael-Tu/radioML/blob/master/notebooks/ecc-v1.ipynb)

- "linear activation function" for both FFN and CNN seem to work better
- "sigmoid" on the last fully connected layer
- tends to set all bytes to 0s or 1s

# Feedforward Neural Network

## Hyperparameters
- ACTIVATIONS = [None, "relu"]
- L2 = 1e-7
- BATCH_SIZE = 128
- EPOCHS = 100
- VALIDATION_RATIO = 0.2
- TRAIN_TEST_RATIO = 0.2

## Loss
Layer Outputs | Hidden Layer Activation | loss
--- | --- | ---
[32,100] | None                 | 39.1067
[32,100] | relu                 | 39.3381
[64,100] | None                 | 40.0048
[64,100] | relu                 | 39.3159
[128,100] | None                | 39.7810
[128,100] | relu                | 38.8710
[256,100] | None                | 39.4921
[256,100] | relu                | 39.0508
[32,32,100] | None              | 39.0020
[32,32,100] | relu              | 39.1937
[32,64,100] | None              | 39.2079
[32,64,100] | relu              | 39.2833
[64,64,100] | None              | 40.3151
[64,64,100] | relu              | 36.6774
[64,128,100] | None             | 39.8548
[64,128,100] | relu             | 36.5746
[128,128,100] | None            | 39.2095
[128,128,100] | relu            | **36.5591**
[128,256,100] | None            | 39.9813
[128,256,100] | relu            | 36.6901
[256,256,100] | None            | 39.6996
[256,256,100] | relu            | 37.2758
[32,128,128,100] | None         | 38.5710
[32,128,128,100] | relu         | 37.6992
[32,128,256,100] | None         | 38.8147
[32,128,256,100] | relu         | 36.9992
[32,256,256,100] | None         | 38.5742
[32,256,256,100] | relu         | 37.4948
[32,128,256,128,100] | None     | 39.4413
[32,128,256,128,100] | relu     | 36.9464
[32,128,128,128,100] | None     | 38.7067
[32,128,128,128,100] | relu     | 36.7889
[1024,512,100] | None           | 39.4456
[1024,512,100] | relu           | 36.9075
[512,256,100]  | None           | 38.6782
[512,256,100]  | relu           | 37.6750
[256,128,100]  | None           | 39.1917
[256,128,100]  | relu           | 36.8278
[128,100,100]  | None           | 39.7758
[128,100,100]  | relu           | 37.3627

## Example of Best Model

```
average loss for viterbi decoding is: 33.9929; acc 0.8366
average loss for model is: 36.7734; acc 0.8232
## Random Example ##
ground_truth
 [1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1.
 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.
 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.
 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1.
 1. 0. 0. 1.]
prediction_thresholded
 [1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1.
 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1.
 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.
 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0.
 1. 0. 0. 1.]
DIFFERENCE (acc 0.4600%)
 [0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0.
 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0.
 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.
 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1.
 0. 0. 0. 0.]
dummy baseline (set every bit to the most occuring bit) can get 50.0% accurate
```

# CNN

```
true
 [0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0.
 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0.
 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1.]
pred
 [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.
 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.
 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1.
 1. 1. 1. 1.]
accu 70%
```



