{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael-tu/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael-tu/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Activation, Dropout, BatchNormalization, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, UpSampling1D\n",
    "from keras.layers import SimpleRNN, RNN, LSTM, Embedding\n",
    "from keras.optimizers import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pkl(path):\n",
    "    return pickle.load(open(path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    keys = ['message_seqs', 'encoded_seqs', 'noisy_seqs', 'viterbi_decoded_seqs']\n",
    "    x = data[keys[2]] # noisy sequences will be our input to our models\n",
    "    y = data[keys[0]]\n",
    "    y_viterbi_decoded = data[keys[3]]\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    y_viterbi_decoded = np.array(y_viterbi_decoded)\n",
    "    \n",
    "    return x, y, y_viterbi_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    return load_data(load_pkl(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pkl_paths_from_folder(dir_path, recursive=False):\n",
    "    if recursive:\n",
    "        return np.sort(list(filter(lambda x: \".pkl\" in x,\n",
    "                            [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(dir_path)) \n",
    "                                                 for f in fn])))\n",
    "    else:\n",
    "        return np.sort([dir_path + \"/\" + x for x in os.listdir(dir_path) if \".pkl\" in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_param_from_filename(filename, param):\n",
    "    split_token = \"_\"\n",
    "    if param == \"k\":\n",
    "        split_token = \"_k\"\n",
    "    elif param == \"p\":\n",
    "        split_token = \"_p\"\n",
    "    elif param == \"r\":\n",
    "        split_token = \"_r\"\n",
    "    elif param == \"l\":\n",
    "        split_token = \"_l\"\n",
    "    elif param == \"e\":\n",
    "        split_token = \"_e\"\n",
    "    elif param == \"n\":\n",
    "        split_token = \"_n\"\n",
    "    else:\n",
    "        raise ValueError(\"unrecognized parameter\", param)\n",
    "        \n",
    "    res = filename.split(\"/\")[-1].split(split_token)[1].split(\"_\")[0]\n",
    "    if \".pkl\" in res:\n",
    "        res = res.split(\".pkl\")[0]\n",
    "    \n",
    "    return float(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_ratio=0.2):\n",
    "    indicies = np.arange(len(x))\n",
    "    np.random.shuffle(indicies)\n",
    "    \n",
    "    split_inx = int(len(x) * (1-test_ratio))\n",
    "    X_train, X_test = x[indicies[:split_inx]], x[indicies[split_inx:]]\n",
    "    y_train, y_test = y[indicies[:split_inx]], y[indicies[split_inx:]]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group_sort(benchmark, *args):\n",
    "    sorted_indicies = np.argsort(benchmark) \n",
    "    \n",
    "    benchmark = np.array(benchmark)\n",
    "    args = list(map(lambda x: np.array(x), args))\n",
    "    \n",
    "    benchmark_sorted = benchmark[sorted_indicies]\n",
    "    args_sorted = list(map(lambda x: x[sorted_indicies], args))\n",
    "    \n",
    "    return benchmark_sorted, args_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(data):\n",
    "    res = []\n",
    "    for d in data:\n",
    "        res.append([0,0])\n",
    "        res[-1][int(d)] = 1\n",
    "    return np.array(res)\n",
    "\n",
    "def one_hot_datasets(y):\n",
    "    return np.array(list(map(lambda x: one_hot(x), y)))\n",
    "\n",
    "def onehot_to_normal(dataset):\n",
    "    return np.argmax(dataset, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variable_desc = {\n",
    "    \"k\": \"message length (K-bit)\",\n",
    "    \"p\": \"prob P of Bernoulli distribution\",\n",
    "    \"e\": \"corruption probability E\",\n",
    "    \"l\": \"constraint length L\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_from_path(path, test_ratio=0.2, X_reshape=None):\n",
    "    X, y, y_viterbi_decoded = load(path)\n",
    "    y_oh = one_hot_datasets(y)\n",
    "    \n",
    "    if X_reshape:\n",
    "        X = X.reshape(X_reshape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_ratio=test_ratio)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X, y, y_oh, y_viterbi_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data_from_path(path, test_ratio=0.2, X_reshape=None):\n",
    "    X, y, y_viterbi_decoded = load(path)\n",
    "    y_oh = one_hot_datasets(y)\n",
    "    \n",
    "    if X_reshape:\n",
    "        total_shape = np.product(X_reshape[1:])\n",
    "        if total_shape != X.shape[1]:\n",
    "            X = X[:,:total_shape]\n",
    "        X = X.reshape(X_reshape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_ratio=test_ratio)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X, y, y_oh, y_viterbi_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"adam\": Adam, \n",
    "    \"sgd\":  SGD\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_fnn_param = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epoch\": 25,\n",
    "    \"verbose\": 2,\n",
    "    \"validation_ratio\": 0.2,\n",
    "    \"activation\": \"relu\",\n",
    "    \"l2_strength\": 0.01,\n",
    "    \"learing_rate\": 0.015,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    \"loss_fn\": \"binary_crossentropy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_fnn_architecture = [32,64,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution 1D Nerual Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_cnn_param = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epoch\": 25,\n",
    "    \"verbose\": 2,\n",
    "    \"validation_ratio\": 0.2,\n",
    "    \"activation\": \"relu\",\n",
    "    \"dropout_keep_prob\": 0.5,\n",
    "    \"learing_rate\": 0.01,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"conv_padding\": \"same\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    \"loss_fn\": \"binary_crossentropy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliable Layers:\n",
    "\n",
    "Dense, Input, Activation, Dropout, BatchNormalization, Flatten, Reshape\n",
    "Conv2D, MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "Conv1D, MaxPooling1D, AveragePooling1D, UpSampling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "default_cnn_architecture = [\n",
    "    (\"conv1d\", dict(f=16,k=3,s=1)),\n",
    "    (\"conv1d\", dict(f=16,k=3,s=1)),\n",
    "    (\"conv1d\", dict(f=16,k=3,s=1)),\n",
    "    (\"conv1d\", dict(f=16,k=3,s=1)),\n",
    "    (\"flatten\"),\n",
    "    (\"dense\", 200),\n",
    "    (\"dropout\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fnn(input_shape, \n",
    "              output_shape, \n",
    "              architecture=default_fnn_architecture, \n",
    "              param=default_fnn_param):\n",
    "    \"\"\"\n",
    "    Train a FNN model with INPUT_SHAPE and OUTPUT_SHAPE using ARCHITECTURE and PARAM.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    for i, output_units in enumerate(architecture):\n",
    "        if i == 0:\n",
    "            model.add(Dense(output_units, activation=param[\"activation\"],\n",
    "                        input_shape=input_shape,\n",
    "                        kernel_regularizer=regularizers.l2(param[\"l2_strength\"])))\n",
    "        else:\n",
    "            model.add(Dense(output_units, activation=param[\"activation\"],\n",
    "                        kernel_regularizer=regularizers.l2(param[\"l2_strength\"])))\n",
    "    \n",
    "    model.add(Dense(np.product(output_shape), activation=\"sigmoid\"))\n",
    "    model.add(Reshape(output_shape))\n",
    "    \n",
    "    opt = optimizers[param[\"optimizer\"]](lr=param[\"learing_rate\"])\n",
    "    model.compile(optimizer=opt, loss=param[\"loss_fn\"], metrics=param[\"metrics\"])\n",
    "    \"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_fnn(model, x, y, param=default_fnn_param, shuffle=True):\n",
    "    model.fit(x, y, batch_size=param[\"batch_size\"], epochs=param[\"epoch\"], shuffle=shuffle,\n",
    "              verbose=param[\"verbose\"], validation_split=param[\"validation_ratio\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_shape, \n",
    "              output_shape, \n",
    "              architecture=default_cnn_architecture, \n",
    "              param=default_cnn_param):\n",
    "    \"\"\"\n",
    "    Train a CNN model with INPUT_SHAPE and OUTPUT_SHAPE using ARCHITECTURE and PARAM.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    for i, layer in enumerate(architecture):\n",
    "        if layer[0] == \"conv1d\":\n",
    "            model.add(Conv1D(filters=16, kernel_size=3, strides=1,padding=PADDING, activation=ACTIVATION))\n",
    "        elif layer[0] == \"flatten\":\n",
    "            model.add(Flatten())\n",
    "        elif layer[0] == \"dense\":\n",
    "            model.add(Dense(layer[1], activation=param[\"activation\"]))\n",
    "        elif layer[0] == \"dropout\":\n",
    "            model.add(Dropout(param[\"dropout_keep_prob\"]))\n",
    "        else:\n",
    "            raise ValueError(\"do not recognize layer\", layer[0])\n",
    "    \n",
    "    model.add(Dense(np.product(output_shape), activation=\"sigmoid\"))\n",
    "    model.add(Reshape(output_shape))\n",
    "    \n",
    "    opt = optimizers[param[\"optimizer\"]](lr=param[\"learing_rate\"])\n",
    "    model.compile(optimizer=opt, loss=param[\"loss_fn\"], metrics=param[\"metrics\"])\n",
    "    \"\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_cnn(model, x, y, param=default_fnn_param, shuffle=True):\n",
    "    model.fit(x, y, batch_size=param[\"batch_size\"], epochs=param[\"epoch\"], shuffle=shuffle,\n",
    "              verbose=param[\"verbose\"], validation_split=param[\"validation_ratio\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred, argmax=False, viterbi=False):\n",
    "    y_pred_binary = y_pred\n",
    "    \n",
    "    if viterbi:\n",
    "        y_pred_binary = [p[:y.shape[1]] for p in y_pred]\n",
    "    elif argmax:\n",
    "        y_pred_binary = np.vstack([np.argmax(p, axis=1) for p in y_pred])\n",
    "        assert y.shape == y_pred_binary.shape, \"prediction and ground truth must \\\n",
    "        have same shape\\nExpected:{0} Actual:{1}\".format(y.shape, y_pred_binary.shape)\n",
    "    \n",
    "    return np.sum(y == y_pred_binary) / np.product(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_accuracy(model, x, y, argmax=False):\n",
    "    if argmax:\n",
    "        y_pred = model.predict(x)\n",
    "        return accuracy(y, y_pred, argmax)\n",
    "    else:\n",
    "        return model.evaluate(x, y, verbose=0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_vs_truth(model, x, y, n_show=2, y_truth_is_onehot=False):\n",
    "    indicies = np.random.randint(low=0, high=x.shape[0], size=n_show)\n",
    "    y_pred = model.predict(x[indicies])\n",
    "    y_pred_binary = np.vstack([np.argmax(p, axis=1) for p in y_pred])\n",
    "    \n",
    "    truth = y[indicies].astype(y_pred_binary.dtype)\n",
    "    \n",
    "    if y_truth_is_onehot:\n",
    "        truth = onehot_to_normal(truth)\n",
    "    \n",
    "    for i in range(len(truth)):\n",
    "        print(\"\\n############## Example {0} ##############\".format(i))\n",
    "        print(\"Expected:\\n\", truth[i])\n",
    "        print(\"Actual:\\n\", y_pred_binary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_acc(title, acc):\n",
    "    epoch = np.arange(1,len(acc)+1,1)\n",
    "    \n",
    "    plt.figure(figsize=(20,6))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.plot(epoch, acc, \"green\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Log Log {0}\".format(title))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.plot(epoch, np.log(-np.log(acc)), \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
